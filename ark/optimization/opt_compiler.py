import ast
from copy import copy
from typing import Callable, Iterable

import jax
import jax.numpy as jnp

from ark.cdg.cdg import CDG, CDGElement, CDGNode
from ark.compiler import ArkCompiler, mk_arg, set_ctx
from ark.optimization.base_module import BaseAnalogCkt
from ark.specification.attribute_def import AttrDefMismatch, Trainable
from ark.specification.attribute_type import AnalogAttr, DigitalAttr
from ark.specification.specification import CDGSpec

ark_compiler = ArkCompiler()


def mk_var_generator(name: str):
    return lambda: ast.Name(id=name)


def mk_assign(target: ast.expr, value: ast.expr):
    """target = value"""
    return ast.Assign(
        targets=[set_ctx(target, ast.Store)],
        value=set_ctx(value, ast.Load),
    )


def mk_call(fn: ast.expr, args: list[ast.expr]):
    """fn(*args)"""
    return ast.Call(
        func=fn,
        args=args,
        keywords=[],
    )


def mk_tuple(elts: list[ast.Name | ast.Constant]):
    return ast.Tuple(elts=elts)


def mk_list(lst: list[ast.Name | ast.Constant]):
    return ast.List(elts=lst)


def mk_arr_access(lst: ast.Name, idx: ast.expr):
    return ast.Subscript(
        value=lst,
        slice=idx,
    )


def mk_list_val_expr(lst: Iterable):
    """make the list value to be expressions"""
    lst_expr = []
    for val in lst:
        if isinstance(val, ast.expr):
            lst_expr.append(val)
        elif isinstance(val, (int, float)) or val is None:
            lst_expr.append(ast.Constant(value=val))
        else:
            raise ValueError(f"Unknown type {type(val)} to be converted in the list")
    return lst_expr


def mk_jnp_call(args: list, call_fn: str):
    """jnp.call_fn(*args)"""

    return ast.Call(
        func=ast.Attribute(value=ast.Name(id="jnp"), attr=call_fn),
        args=mk_list_val_expr(args),
        keywords=[],
    )


def mk_jax_random_call(args: list, call_fn: str):
    """jax.random.call_fn(*args)"""
    return ast.Call(
        func=ast.Attribute(
            value=ast.Attribute(value=ast.Name(id="jax"), attr="random"), attr=call_fn
        ),
        args=mk_list_val_expr(args),
        keywords=[],
    )


def mk_jnp_arr_access(arr: ast.Name, idx: ast.expr):
    """arr.at[idx]"""
    return ast.Subscript(
        value=ast.Attribute(value=arr, attr="at"),
        slice=idx,
    )


def mk_jnp_assign(arr: ast.Name, idx: ast.expr, val: ast.Name | ast.Constant):
    """
    arr = arr.at[idx].set(val)
    """
    return mk_assign(
        target=arr,
        value=ast.Call(
            func=ast.Attribute(
                value=mk_jnp_arr_access(copy(arr), idx),
                attr="set",
            ),
            args=[val],
            keywords=[],
        ),
    )


def cnt_n_mismatch_attr(cdg: CDG) -> int:
    cnt = 0
    ele: CDGElement
    for ele in cdg.elements:
        for val in ele.attr_def.values():
            if isinstance(val, AttrDefMismatch):
                cnt += 1
    return cnt


def cdg_to_initial_states(cdg: CDG, node_mapping: dict[str, int]) -> list[float]:
    """
    Extract the initial states from a CDG

    Args:
        cdg (CDG): the CDG
        node_mapping (dict[str, int]): mapping from node name to the index in the
        initial states array (generated by the compiler)

    Returns:
        list[float]: the initial states array
    """
    node_to_init_state = cdg.node_to_init_state
    assert (
        node_to_init_state.keys() == node_mapping.keys()
    ), "Node-to-initial-state mapping mismatch"
    n_states = cdg.total_1st_order_states
    initial_states = [None for _ in range(n_states)]
    node: CDGNode
    for node, states in node_to_init_state.items():
        for order, s in enumerate(states):
            initial_states[node_mapping[node] + order] = s
    return initial_states


def cdg_to_switch_array(cdg: CDG, switch_map: dict[str, int]) -> list[int]:
    """
    Extract the switch array from a CDG

    Args:
        cdg (CDG): the CDG
        switch_map (dict[str, int]): mapping from switch name to the index in the
        switch array (generated by the compiler)

    Returns:
        list[int]: the switch array
    """
    n_switch = len(switch_map)
    switch_vals = [0 for _ in range(n_switch)]
    assert (
        cdg.switch_to_val.keys() == switch_map.keys()
    ), "Edge-to-switch mapping mismatch"
    for switch, val in cdg.switch_to_val.items():
        switch_vals[switch_map[switch]] = val
    return switch_vals


def base_readout(y: jax.Array, idx: list[int]):
    return y[:, idx]


def check_trainable_consisitency(cdg: CDG):
    """Check if the trainable attributes in the CDG are consistent

    1. their indices should be 0, 1, 2, ... without skipping
    2. If trainable parameter is shared, their range and type should be identical.

    Returns:
        size of the trainable parameters
    """

    trainable_types: dict[int, tuple[CDGElement, str]] = {}
    for ele in cdg.elements:
        for attr, val in ele.attrs.items():
            if isinstance(val, Trainable):
                if val.idx not in trainable_types:
                    trainable_types[val.idx] = (ele, attr)
                else:
                    cur_attr_type = ele.attr_def[attr].attr_type
                    stored_ele, stored_attr = trainable_types[val.idx]
                    stored_attr_type = stored_ele.attr_def[stored_attr].attr_type
                    if isinstance(cur_attr_type, type(stored_attr_type)):
                        raise ValueError(
                            f"Shared trainable parameter {val.idx}  has inconsistent type "
                            + f"in {ele.name}.{attr} and {stored_ele.name}.{stored_attr}"
                        )
                    if isinstance(cur_attr_type, AnalogAttr):
                        if cur_attr_type.val_range != stored_attr_type.val_range:
                            raise ValueError(
                                f"Shared trainable parameter {val.idx}  has inconsistent range "
                                + f"in {ele.name}.{attr} and {stored_ele.name}.{stored_attr}"
                            )
                    elif isinstance(cur_attr_type, DigitalAttr):
                        if cur_attr_type.val_choices != stored_attr_type.val_choices:
                            raise ValueError(
                                f"Shared trainable parameter {val.idx}  has inconsistent choices "
                                + f"in {ele.name}.{attr} and {stored_ele.name}.{stored_attr}"
                            )
                    else:
                        raise ValueError(
                            f"Unsupport attribute type {type(cur_attr_type)} for trainable parameter"
                        )
    max_id = max(trainable_types.keys())
    for i in range(max_id + 1):
        if i not in trainable_types:
            raise ValueError(f"Trainable parameter {i} is missing")
    return max_id + 1


class OptCompiler:

    SELF = "self"
    SWITCH = "switch"
    MISMATCH_SEED = "mismatch_seed"
    NOISE_SEED = "noise_seed"
    A_TRAINABLE = "a_trainable"
    D_TRAINABLE = "d_trainable"
    NORMALIZE_MIN, NORMALIZE_MAX = -1, 1

    def __init__(self) -> None:
        pass

    def compile(
        self,
        prog_name: str,
        cdg: CDG,
        cdg_spec: CDGSpec,
        readout_nodes: list[CDGNode] = None,
        normalize_weight: bool = True,
        do_clipping: bool = True,
    ) -> type:
        """Compile the cdg to an equinox.Module.

        Args:
            prog_name (str): name of the program
            cdg (CDG): the dynamical graph
            cdg_spec (CDGSpec): the specification of the dynamical graph
            normalize weight (bool, optional): whether to normalize the trainable
            parameters or not. If true, the traianble weights are assumed to within
            [-1, 1]. Defaults to True.
            do_clipping (bool, optional): whether to clip the value within the range
            specified in ``cdg_spec``. Defaults to True.

        Returns:
            type: the compiled module.
        """

        # Check the consistency of the trainable attributes
        trainable_len = check_trainable_consisitency(cdg)

        (ode_term, noise_term), node_mapping, switch_map, num_attr_map, fn_attr_map = (
            ark_compiler.compile_odeterm(cdg, cdg_spec)
        )

        self.mm_used_idx = 0
        namespace = {"jax": jax, "jnp": jnp}

        # Usefule constants
        args_len = len(switch_map) + sum(len(x) for x in num_attr_map.values())
        fargs_len = sum(len(x) for x in fn_attr_map.values())
        n_mismatch = cnt_n_mismatch_attr(cdg)

        # Input variables ast expr
        self_expr_gen = mk_var_generator(self.SELF)
        switch_expr_gen = mk_var_generator(self.SWITCH)
        mismatch_seed_expr_gen = mk_var_generator(self.MISMATCH_SEED)
        noise_seed_expr_gen = mk_var_generator(self.NOISE_SEED)

        #  Function arguments
        fn_args = [None for _ in range(fargs_len)]

        # Common variables ast expr
        args_expr_gen = mk_var_generator("args")
        a_trainable_expr_gen = mk_var_generator(self.A_TRAINABLE)
        mm_prng_key_expr_gen = mk_var_generator("mm_key")
        mm_arr_expr_gen = mk_var_generator("mm_arr")

        stmts = []
        # Assign self.trainable to trainable for readability
        # and clipping the trainable values
        # trainable = jnp.clip(self.trainable, CLIPPED_MIN, CLIPPED_MAX)
        a_trainable_expr_rhs = self._clip_and_denormalize_trainable(
            cdg, trainable_len, self_expr_gen, normalize_weight, do_clipping
        )
        stmts.append(
            mk_assign(
                a_trainable_expr_gen(),
                a_trainable_expr_rhs,
            )
        )
        # Initialize the jnp arrays
        init_arr = mk_jnp_call(args=[args_len], call_fn="zeros")
        stmts.append(mk_assign(args_expr_gen(), init_arr))

        # Initialize the switch array (switch array is always the beginning of the args)
        # args = args.at[:switch_args_len].set(switch)
        switch_args_len = len(switch_map)
        stmts.append(
            mk_jnp_assign(
                arr=args_expr_gen(),
                idx=ast.Slice(upper=ast.Constant(value=switch_args_len)),
                val=switch_expr_gen(),
            )
        )

        # Initialize the mismatch array
        stmts.append(
            mk_assign(
                mm_prng_key_expr_gen(),
                mk_jax_random_call(args=[mismatch_seed_expr_gen()], call_fn="PRNGKey"),
            )
        )
        mismatch_arr = mk_jax_random_call(
            args=[mm_prng_key_expr_gen(), mk_list(mk_list_val_expr([n_mismatch]))],
            call_fn="normal",
        )
        stmts.append(
            mk_assign(
                mm_arr_expr_gen(),
                mismatch_arr,
            )
        )
        ele: CDGElement
        for ele in cdg.elements:
            assert ele.name in num_attr_map or ele.name in fn_attr_map
            if ele.name in fn_attr_map:
                fn_attr_to_idx = fn_attr_map[ele.name]
            if ele.name in num_attr_map:
                num_attr_to_idx = num_attr_map[ele.name]
            for attr, val in ele.attrs.items():
                if isinstance(val, Callable):
                    # Handle function attribute
                    assert attr in fn_attr_to_idx
                    fn_args[fn_attr_to_idx[attr]] = val
                else:
                    assert attr in num_attr_to_idx
                    if isinstance(val, Trainable):
                        # Handle trainable attribute
                        attr_type = ele.attr_def[attr].attr_type
                        if isinstance(attr_type, AnalogAttr):
                            val: Trainable
                            trainable_id = val.idx
                            val_expr = mk_arr_access(
                                a_trainable_expr_gen(), ast.Constant(value=trainable_id)
                            )
                        elif isinstance(attr_type, DigitalAttr):
                            raise NotImplementedError

                        else:
                            raise ValueError(
                                f"Unsupported trainable attribute type {type(attr_type)}"
                            )
                    else:
                        # Handle fixed attribute
                        val_expr = ast.Constant(value=val)

                    # If the attribute is mismatched, apply the mismatch
                    if isinstance(ele.attr_def[attr], AttrDefMismatch):
                        val_expr = self._mk_mismatch_expr(
                            orig_expr=val_expr,
                            mm_arr_expr=mm_arr_expr_gen(),
                            attr_def=ele.attr_def[attr],
                        )

                    stmts.append(
                        mk_jnp_assign(
                            arr=args_expr_gen(),
                            idx=ast.Constant(value=num_attr_to_idx[attr]),
                            val=val_expr,
                        )
                    )

        # Return the args
        stmts.append(set_ctx(ast.Return(value=args_expr_gen()), ast.Load))

        # Compile the statements to make_args(self, switch, mismatch_seed) function
        make_args_fn = ast.FunctionDef(
            name="make_args",
            args=ast.arguments(
                posonlyargs=[],
                args=[
                    mk_arg(self.SELF),
                    mk_arg(self.SWITCH),
                    mk_arg(self.MISMATCH_SEED),
                ],
                vararg=None,
                kwonlyargs=[],
                kw_defaults=[],
                kwarg=None,
                defaults=[],
            ),
            body=stmts,
            decorator_list=[],
        )
        module = ast.Module([make_args_fn], type_ignores=[])
        module = ast.fix_missing_locations(module)
        print(ast.unparse(module))
        exec(
            compile(source=module, filename="tmp.py", mode="exec"),
            namespace,
        )

        ode_fn = lambda self, t, y, args: ode_term(t, y, args, fn_args)
        noise_fn = lambda self, t, y, args: noise_term(t, y, args, fn_args)

        # Setup readout
        if readout_nodes is None:
            idx = [i for i in range(cdg.total_1st_order_states)]
        else:
            for node in readout_nodes:
                assert node.order >= 1, "Readout node must have order >= 1"
                # If the node has higher order, only show the lowest order
                idx = [node_mapping[node.name] for node in readout_nodes]
        readout_fn = lambda self, y: base_readout(y, idx)

        cdg_to_init_state_fn = lambda cdg: cdg_to_initial_states(cdg, node_mapping)

        cdg_to_switch_array_fn = lambda cdg: cdg_to_switch_array(cdg, switch_map)

        opt_module = type(
            prog_name,
            (BaseAnalogCkt,),
            {
                "ode_fn": ode_fn,
                "noise_fn": noise_fn,
                "make_args": namespace["make_args"],
                "readout": readout_fn,
                "cdg_to_initial_states": cdg_to_init_state_fn,
                "cdg_to_switch_array": cdg_to_switch_array_fn,
            },
        )

        return opt_module

    def _clip_and_denormalize_trainable(
        self,
        cdg: CDG,
        trainable_len: int,
        self_expr_gen: Callable,
        normalized: bool,
        clip: bool,
    ) -> ast.expr:
        # The shared trainables are checked to have consistent range
        # Can use the first trainable to get the range for clipping and normalize

        def jnp_clip_expr(val: ast.expr, min_val, max_val):
            return mk_jnp_call(
                args=[
                    val,
                    min_val,
                    max_val,
                ],
                call_fn="clip",
            )

        self_dot_trainable = ast.Attribute(value=self_expr_gen(), attr=self.A_TRAINABLE)

        if not normalized and not clip:
            return self_dot_trainable

        min_arr = [None for _ in range(trainable_len)]
        max_arr = [None for _ in range(trainable_len)]
        for ele in cdg.elements:
            for attr, val in ele.attrs.items():
                if isinstance(val, Trainable):
                    attr_type = ele.attr_def[attr].attr_type
                    if isinstance(attr_type, AnalogAttr):
                        min_val, max_val = (
                            attr_type.val_range.min,
                            attr_type.val_range.max,
                        )
                        min_arr[val.idx] = min_val
                        max_arr[val.idx] = max_val

        if normalized:
            # Traianble weights are within [self.NORMALIZE_MIN, self.NORMALIZE_MAX]

            trainable = self_dot_trainable
            if clip:
                trainable = jnp_clip_expr(
                    self_dot_trainable,
                    self.NORMALIZE_MIN,
                    self.NORMALIZE_MAX,
                )

            norm_diff, half_norm_sum = (
                self.NORMALIZE_MAX - self.NORMALIZE_MIN,
                (self.NORMALIZE_MAX + self.NORMALIZE_MIN) / 2,
            )
            min_val, max_val = jnp.array(min_arr, dtype=jnp.float64), jnp.array(
                max_arr, dtype=jnp.float64
            )
            range_diff, half_range_sum = max_val - min_val, (max_val + min_val) / 2

            # Denormalize the trainable values to the original range
            # Denormalization: [normalized_min, normalized_max] -> [min, max]
            # val = (norm_val - (half_norm_sum)) * range_diff / norm_diff + (half_range_sum)
            diff_range_over_norm = range_diff / norm_diff
            diff_range_over_norm_jnp_arr = mk_jnp_call(
                [mk_list(mk_list_val_expr(diff_range_over_norm.tolist()))],
                call_fn="array",
            )
            half_range_sum_jnp_arr = mk_jnp_call(
                [mk_list(mk_list_val_expr(half_range_sum.tolist()))],
                call_fn="array",
            )

            trainable = ast.BinOp(
                left=ast.BinOp(
                    left=ast.BinOp(
                        left=trainable,
                        op=ast.Sub(),
                        right=ast.Constant(value=half_norm_sum),
                    ),
                    op=ast.Mult(),
                    right=diff_range_over_norm_jnp_arr,
                ),
                op=ast.Add(),
                right=half_range_sum_jnp_arr,
            )

        else:  # Only clipping
            min_arr = mk_jnp_call([mk_list(mk_list_val_expr(min_arr))], call_fn="array")
            max_arr = mk_jnp_call([mk_list(mk_list_val_expr(max_arr))], call_fn="array")

            trainable = jnp_clip_expr(
                self_dot_trainable,
                min_arr,
                max_arr,
            )

        return trainable

    def _mk_mismatch_expr(
        self, orig_expr: ast.expr, mm_arr_expr: ast.Name, attr_def: AttrDefMismatch
    ):
        """
        Return the expression after adding the mismatch
        """
        if attr_def.std:
            # orig_expr + std*mm_arr_expr[self.mm_used_idx])
            std_expr = ast.Constant(value=attr_def.std)
            mm_expr = ast.BinOp(
                left=orig_expr,
                op=ast.Add(),
                right=ast.BinOp(
                    left=std_expr,
                    op=ast.Mult(),
                    right=mk_arr_access(
                        mm_arr_expr, ast.Constant(value=self.mm_used_idx)
                    ),
                ),
            )
        elif attr_def.rstd:
            # orig_expr * (1 + std*mm_arr_expr[self.mm_used_idx])
            rstd_expr = ast.Constant(value=attr_def.rstd)
            mm_expr = ast.BinOp(
                left=orig_expr,
                op=ast.Mult(),
                right=ast.BinOp(
                    left=ast.Constant(value=1),
                    op=ast.Add(),
                    right=ast.BinOp(
                        left=rstd_expr,
                        op=ast.Mult(),
                        right=mk_arr_access(
                            mm_arr_expr, ast.Constant(value=self.mm_used_idx)
                        ),
                    ),
                ),
            )
        else:
            raise ValueError("Must specify either rstd or std")

        self.mm_used_idx += 1
        return mm_expr
